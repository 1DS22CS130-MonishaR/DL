LAB PROGRAM 1

import tensorflow as tf
scalar=tf.constant(6)
print("scalar: \n",scalar)

print("\n")
#1D tensor
vector=tf.constant([1,2,3])
print("vector: \n",vector)

print("\n")
#2D tensor
matrix=tf.constant([[1,2],[3,4]])
print("matrix: \n",matrix)

print("\n")
#3D tensor
tensor=tf.constant([[[1,2],[3,4]],[[5,6],[7,8]]])
print("tensor:\n",tensor)

print("\n")
#addition of tensor
tensor_a=tf.constant([[1,2],[3,4]])
tensor_b=tf.constant([[1,2],[3,4]])
addition=tf.add(tensor_a,tensor_b)
print("addition: \n",addition)

print("\n")
#matrix-multiplication
tensor_a=tf.constant([[1,2],[3,4]])
tensor_b=tf.constant([[1,2],[3,4]])
matrixmultiplication=tf.matmul(tensor_a,tensor_b)
print("matrix multiplication: \n",matrixmultiplication)

print("\n")
#reshape
tensor_a=tf.constant([[1,2],[3,4],[5,6]])
reshape=tf.reshape(tensor_a,[2,3])
print("reshape:\n",reshape)

print("\n")
#element-wise multiplication
tensor_a=tf.constant([[1,2],[3,4]])
tensor_b=tf.constant([[1,2],[3,4]])
elementwisemultiplication=tf.multiply(tensor_a,tensor_b)
print("elementwise multiplication: \n",elementwisemultiplication)

print("\n")
#sequential model
model=tf.keras.Sequential([
    tf.keras.layers.Dense(64,activation="relu",input_shape=(10,1)),
    tf.keras.layers.Dense(64,activation="relu"),
    tf.keras.layers.Dense(1,activation="sigmoid")

])
model.summary()





LAB PROGRAM 2



import tensorflow as tf
from tensorflow import keras
from keras import layers
import numpy as np
import matplotlib.pyplot as plt

#load the data
cifar100=tf.keras.datasets.cifar100

#distribute th data into train and validation
(x_train,y_train),(x_val,y_val)=cifar100.load_data()
print(x_train.shape,y_train.shape)
print(x_val.shape,y_val.shape)

import warnings
warnings.filterwarnings('ignore')

y_train=tf.one_hot(y_train,depth=100,dtype=tf.float64)
y_val=tf.one_hot(y_val,depth=100,dtype=tf.float64)

y_train=tf.squeeze(y_train)
y_val=tf.squeeze(y_val)

model=tf.keras.models.Sequential([
   layers.Conv2D(16,(3,3),activation="relu",padding="same",input_shape=(32,32,3)),
   layers.Conv2D(32,(3,3),activation="relu",padding="same"),
   layers.Conv2D(64,(3,3),activation="relu",padding="same"),
   layers.MaxPooling2D(2,2),
   layers.Conv2D(128,(3,3),activation="relu",padding="same"),
   layers.Flatten(),
   layers.Dense(256,activation="relu"),
   layers.BatchNormalization(),
   layers.Dense(256,activation="relu"),
   layers.Dropout(0.3),
   layers.BatchNormalization(),
   layers.Dense(100,activation="softmax")
])

model.summary()

model.compile(
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
    optimizer="Adam",
    metrics=["accuracy"]
)

history=model.fit(
    x_train,y_train,
    epochs=5,
    validation_data=(x_val,y_val),
    batch_size=100,
    verbose=1
)

loss,accuracy=model.evaluate(x_val,y_val)
print("loss: ",loss)
print("accuarcy: ",accuracy)



LAB PROGRAM 6


import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras import layers
from tensorflow.keras.layers import Input,Conv2D,UpSampling2D,MaxPooling2D
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Model

#load the data
(x_train,_),(x_test,_)=mnist.load_data()

#normalize the data
x_train=x_train.astype('float32')/255.
x_test=x_test.astype('float32')/255.

#reshape to 28,28
x_train=np.reshape(x_train,(len(x_train),28,28,1))
x_test=np.reshape(x_test,(len(x_test),28,28,1))

#add the noise to data
noise_factor=0.5
x_train_noisy=x_train+noise_factor*np.random.normal(loc=0.0,scale=1.0,size=x_train.shape)
x_test_noisy=x_test+noise_factor*np.random.normal(loc=0.0,scale=1.0,size=x_test.shape)

#build the model
input_img=Input(shape=(28,28,1))
x=Conv2D(32,(3,3),activation="relu",padding="same")(input_img)
x=MaxPooling2D((2,2),padding="same")(x)
x=Conv2D(32,(3,3),activation="relu",padding="same")(x)
encoder=MaxPooling2D((2,2),padding="same")(x)

x=Conv2D(32,(3,3),activation="relu",padding="same")(encoder)
x=UpSampling2D((2,2))(x)
x=Conv2D(32,(3,3),activation="relu",padding="same")(x)
x=UpSampling2D((2,2))(x)
decoder=Conv2D(1,(3,3),activation="sigmoid",padding="same")(x)

#compile
autoencoder=Model(input_img,decoder)
autoencoder.compile(optimizer="adam",loss="binary_crossentropy")

#train
autoencoder.fit(x_train_noisy,x_train,epochs=5,shuffle=True,validation_data=(x_test_noisy,x_test),batch_size=128)

#predict
decoded_img=autoencoder.predict(x_test_noisy)

#plot the graph
n=10
plt.figure(figsize=(20,4))
for i in range(n):
  ax=plt.subplot(3,n,i+1)
  plt.imshow(x_test_noisy[i].reshape(28,28),cmap="gray")
  ax.axis("off")

  ax=plt.subplot(3,n,i+n+1)
  plt.imshow(decoded_img[i].reshape(28,28),cmap="gray")
  ax.axis("off")

  ax=plt.subplot(3,n,i+2*n+1)
  plt.imshow(x_test[i].reshape(28,28),cmap="gray")
  ax.axis("off")
plt.tight_layout()
plt.show()






LAB PROGRAM 7
import numpy as np
import pandas as pd
import tensorflow as tf

import matplotlib.pyplot as plt

url="https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv"
df=pd.read_csv(url,parse_dates=["Date"])
print(df)



plt.figure(figsize=(10,4))
plt.plot(df["Temp"])
plt.title("Daily minimum temperature")
plt.xlabel("Dates")
plt.ylabel("temperature")
plt.show()




temps=df["Temp"].values.astype(np.float32)
mean1=temps.mean()
std1=temps.std()
temps=(temps-mean1)/std1



def create_squence(data,window_size):
  X,y=[],[]
  for i in range(len(data)-window_size):
    
    X.append(data[i:i+window_size])
    y.append(data[i+window_size])
  return np.array(X),np.array(y)

window_size=7
X,y=create_squence(temps,window_size)

X=X.reshape((X.shape[0],X.shape[1],1))
split=int(0.8*len(X))
x_train,x_test=X[:split],X[split:]
y_train,y_test=y[:split],y[split:]

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,SimpleRNN
model=Sequential([
    SimpleRNN(64,return_sequences=True,activation="tanh",input_shape=(window_size,1)),
    SimpleRNN(32,activation="tanh"),
    Dense(1)
])


model.compile(optimizer="adam",loss="mse")
model.summary()

from tensorflow.keras.callbacks import EarlyStopping
early_stop=EarlyStopping(monitor="val_loss",patience=5,restore_best_weights=True)

history=model.fit(
    x_train,y_train,
    validation_data=(x_test,y_test),
    callbacks=[early_stop],
    batch_size=32,
    epochs=100
)

predictions=model.predict(x_test)
predicted_values=predictions*std1+mean1
actual_values=y_test*std1+mean1
plt.figure(figsize=(20,4))
plt.plot(predicted_values,label="predicted")
plt.plot(actual_values,label="actual")
plt.legend()
plt.title("---")
plt.show()







LAB PROGRAM 8

import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt

url="https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv"
df=pd.read_csv(url,parse_dates=["Date"])
print(df)



plt.figure(figsize=(10,4))
plt.plot(df["Temp"])
plt.title("Daily minimum temperature")
plt.xlabel("Dates")
plt.ylabel("temperature")
plt.show()




temps=df["Temp"].values.astype(np.float32)
mean1=temps.mean()
std1=temps.std()
temps=(temps-mean1)/std1



def create_squence(data,window_size):
  X,y=[],[]
  for i in range(len(data)-window_size):
    
    X.append(data[i:i+window_size])
    y.append(data[i+window_size])
  return np.array(X),np.array(y)

window_size=7
X,y=create_squence(temps,window_size)

X=X.reshape((X.shape[0],X.shape[1],1))
split=int(0.8*len(X))
x_train,x_test=X[:split],X[split:]
y_train,y_test=y[:split],y[split:]

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,LSTM
model=Sequential([
    LSTM(64,return_sequences=True,activation="tanh",input_shape=(window_size,1)),
    LSTM(32,activation="tanh"),
    Dense(1)
])


model.compile(optimizer="adam",loss="mse")
model.summary()

from tensorflow.keras.callbacks import EarlyStopping
early_stop=EarlyStopping(monitor="val_loss",patience=5,restore_best_weights=True)

history=model.fit(
    x_train,y_train,
    validation_data=(x_test,y_test),
    callbacks=[early_stop],
    batch_size=32,
    epochs=100
)

predictions=model.predict(x_test)
predicted_values=predictions*std1+mean1
actual_values=y_test*std1+mean1
plt.figure(figsize=(10,4))
plt.plot(predicted_values,label="predicted")
plt.plot(actual_values,label="actual")
plt.legend()
plt.title("---")
plt.show()




